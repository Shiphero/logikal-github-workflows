name: Run Python tests
on:
  workflow_call:
    inputs:
      python-version:
        description: The Python version to use
        type: number
        required: false
        default: 3.8
      requirements:
        description: The path to the requirements file
        type: string
        required: false
        default: requirements/dev.txt
      chrome-version:
        description: The Chrome and ChromeDriver version to install
        type: string
        required: false
      hadoop-version:
        description: The Hadoop version to install
        type: string
        required: false
      hadoop-gcs-connector-version:
        description: The Hadoop Google Cloud Storage connector version to install
        type: string
        required: false
      hadoop-aws-module-version:
        description: The Hadoop AWS module version to install
        type: string
        required: false
      aws-java-sdk-version:
        description: The AWS Java SDK bundle version that was used to compile the Hadoop AWS module
        type: string
        required: false

    secrets:
      GCP_TESTING_WORKLOAD_IDENTITY_PROVIDER:
        description: Full identifier of the GitHub workload identity pool provider in Google Cloud
        required: false
      GCP_TESTING_SERVICE_ACCOUNT:
        description: Email of the Google Cloud service account used for running tests
        required: false
      AWS_TESTING_ROLE:
        description: The ARN of the AWS role used for running tests
        required: false
      AWS_TESTING_REGION:
        description: The AWS region to use for authorization
        required: false

concurrency:
  group: ${{ github.workflow }}-python-${{ github.ref || github.run_id }}
  cancel-in-progress: true

jobs:
  run-tests:
    name: Run tests
    runs-on: ubuntu-20.04
    permissions:
      contents: read
      id-token: write
    steps:
      - uses: actions/checkout@v3

      - name: Authenticate to Google Cloud Platform
        uses: google-github-actions/auth@v1
        env:
          RUN_STEP: ${{ secrets.GCP_TESTING_SERVICE_ACCOUNT != '' }}
        if: env.RUN_STEP == 'true'
        with:
          workload_identity_provider: ${{ secrets.GCP_TESTING_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_TESTING_SERVICE_ACCOUNT }}

      - name: Authenticate to Amazon Web Services
        uses: aws-actions/configure-aws-credentials@v1
        env:
          RUN_STEP: ${{ secrets.AWS_TESTING_ROLE != '' }}
        if: env.RUN_STEP == 'true'
        with:
          aws-region: ${{ secrets.AWS_TESTING_REGION }}
          role-to-assume: ${{ secrets.AWS_TESTING_ROLE }}

      - uses: actions/setup-python@v4
        with:
          python-version: ${{ inputs.python-version }}

      - uses: logikal-io/make-orb@v1.0.0
        with:
          requirements: ${{ inputs.requirements }}

      - uses: logikal-io/install-chrome@v1.0.0
        if: inputs.chrome-version != ''
        with:
          version: ${{ inputs.chrome-version }}

      - name: Install Hadoop
        if: inputs.hadoop-version != ''
        run: |-
          # Install Hadoop
          ENDPOINT='https://downloads.apache.org/hadoop/common/hadoop'
          VERSION='${{ inputs.hadoop-version }}'
          echo "Installing Hadoop ${VERSION}"
          wget "${ENDPOINT}-${VERSION}/hadoop-${VERSION}.tar.gz" \
            --progress=dot:giga -O /tmp/hadoop.tar.gz
          sudo tar -xzf /tmp/hadoop.tar.gz --directory /opt/
          sudo mv "/opt/hadoop-${VERSION}" /opt/hadoop
          rm /tmp/hadoop.tar.gz

          # Install Google Cloud Storage connector
          ENDPOINT='https://storage.googleapis.com/hadoop-lib/gcs'
          VERSION='${{ inputs.hadoop-gcs-connector-version }}'
          echo "Installing Google Cloud Storage connector ${VERSION}"
          sudo wget "${ENDPOINT}/gcs-connector-hadoop3-${VERSION}.jar" --progress=dot:giga \
            -O "/opt/hadoop/share/hadoop/common/lib/gcs-connector-hadoop3-${VERSION}.jar"

          # Install AWS module
          ENDPOINT='https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws'
          VERSION='${{ inputs.hadoop-aws-module-version }}'
          echo "Installing Hadoop AWS module ${VERSION}"
          sudo wget "${ENDPOINT}/${VERSION}/hadoop-aws-${VERSION}.jar" --progress=dot:giga \
            -O "/opt/hadoop/share/hadoop/common/lib/hadoop-aws-${VERSION}.jar"

          ENDPOINT='https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle'
          VERSION='${{ inputs.aws-java-sdk-version }}'
          echo "Installing AWS Java SDK ${VERSION}"
          sudo wget "${ENDPOINT}/${VERSION}/aws-java-sdk-bundle-${VERSION}.jar" \
            --progress=dot:giga \
            -O "/opt/hadoop/share/hadoop/common/lib/aws-java-sdk-bundle-${VERSION}.jar"

          # Set environment variables
          export HADOOP_HOME="/opt/hadoop"
          echo "HADOOP_HOME=${HADOOP_HOME}" >> $GITHUB_ENV
          export LD_LIBRARY_PATH="${LD_LIBRARY_PATH:+${LD_LIBRARY_PATH}:}${HADOOP_HOME}/lib/native"
          echo "LD_LIBRARY_PATH=${LD_LIBRARY_PATH}" >> $GITHUB_ENV
          export HADOOP_COMMON_LIBS_JAR_DIR="${HADOOP_HOME}/share/hadoop/common/lib"
          echo "HADOOP_COMMON_LIBS_JAR_DIR=${HADOOP_COMMON_LIBS_JAR_DIR}" >> $GITHUB_ENV

      - name: Run pytest
        run: orb --command pytest

      - name: Upload pytest artifacts
        uses: actions/upload-artifact@v3
        if: failure()
        with:
          name: pytest-artifacts
          path: /tmp/pytest-of-runner/
          if-no-files-found: ignore
          retention-days: 7
